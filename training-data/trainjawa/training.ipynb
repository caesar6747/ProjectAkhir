{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "b6ZIUpwFih7a",
    "outputId": "3f00b5c8-d94c-4f2b-c0a4-0d80a4b9224a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/gdrive')\\n%cd /gdrive\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQ7C1ecfiqcI"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from numpy import array, argmax, random, take\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "import unicodedata\n",
    "import io\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o88uz_9kmSxS"
   },
   "outputs": [],
   "source": [
    "#path_to_file = '/content/fra1.txt'\n",
    "#path_to_file = '/content/drive/My Drive/Colab Notebooks/fra1.txt'\n",
    "path_to_file = 'indo-jawa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OB9IkJCm75L"
   },
   "outputs": [],
   "source": [
    "def unicode_to_acii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itS-QoVznJ64"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_acii(w.lower().strip())\n",
    "\n",
    "# creating a space between a word and the punctuation following it\n",
    "# eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "# Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    " \n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "  \n",
    "# replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "  w = w.strip()\n",
    "# adding a start and an end token to the sentence\n",
    "# so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RXJryIzrr_-s",
    "outputId": "ce3f3d99-96a0-4c55-a365-eaf9b1a47a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> puis je emprunter ce livre ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "fr_sentence = u\"Puis-je emprunter ce livre?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(fr_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWrJ3arQtBpY"
   },
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "uSPFRdBIt9eM",
    "outputId": "6b9c77bc-1a71-4aaa-b51c-2782379e7488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> pergi . <end>\n",
      "<start> tindak . <end>\n",
      "====\n",
      "<start> hai . <end>\n",
      "<start> hai . <end>\n",
      "====\n",
      "<start> lari ! <end>\n",
      "<start> mlayu ! <end>\n",
      "====\n",
      "<start> lari ! <end>\n",
      "<start> mlayu ! <end>\n",
      "====\n",
      "<start> siapa ? <end>\n",
      "<start> who ? <end>\n",
      "====\n",
      "<start> wow ! <end>\n",
      "<start> wuh ! <end>\n",
      "====\n",
      "<start> api ! <end>\n",
      "<start> geni ! <end>\n",
      "====\n",
      "<start> membantu ! <end>\n",
      "<start> tulung ! <end>\n",
      "====\n",
      "<start> melompat . <end>\n",
      "<start> mlumpat . <end>\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "en, fr = create_dataset(path_to_file, None)\n",
    "for s in range(9):\n",
    "    print(en[s])\n",
    "    print(fr[s])\n",
    "    print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGZ6n267uGC4"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQ8FB5U9y1jU"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  \n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sWcBu21y3mn"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 99999\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x00000225017E4E50>\n"
     ]
    }
   ],
   "source": [
    "print(inp_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('inp_lang.pickle', 'wb') as handle:\n",
    "    pickle.dump(inp_lang, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('targ_lang.pickle', 'wb') as handle:\n",
    "    pickle.dump(targ_lang, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "#with open('tokenizer.pickle', 'rb') as handle:\n",
    "#    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W3vexp6c_CCv",
    "outputId": "df371f70-1e51-4ba4-f4e7-0a7cb7100a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 14\n"
     ]
    }
   ],
   "source": [
    "print(max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H87ixlDvzNz0",
    "outputId": "bd763137-dfcb-43a8-ca2f-ecb68b21739a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89999 89999 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size = 0.1 )\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99999, 13)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwvATLwazTRZ"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tom\n"
     ]
    }
   ],
   "source": [
    "print(inp_lang.index_word[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "qmOZS6-w9QFI",
    "outputId": "2a08dc60-3256-430e-eee5-99be15afbeb2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "188 ----> banyu\n",
      "2255 ----> nggambarake\n",
      "1050 ----> cahya\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "195 ----> air\n",
      "5725 ----> memantulkan\n",
      "1275 ----> cahaya\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[100])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PPT0A8Q-9VJE",
    "outputId": "f7401332-b294-4597-85a9-85b25da0cf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 13), (64, 14)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 60\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8428\n",
      "8764\n"
     ]
    }
   ],
   "source": [
    "print(vocab_inp_size)\n",
    "print(vocab_tar_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YDk6DEoiDFsq",
    "outputId": "d948c61a-6e00-4153-8d5c-a8682c4b8c6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 13]), TensorShape([64, 14]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch, target_batch = next(iter(dataset))\n",
    "input_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lDVlaVX_y4v"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    print('cuDNNGRU is Used')\n",
    "    return CuDNNGRU(units, return_sequences=True, \n",
    "                           return_state=True, \n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    print('GRU is used')\n",
    "    return GRU(units, return_sequences=True, \n",
    "                      return_state=True, \n",
    "                      recurrent_activation='sigmoid', \n",
    "                      recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "teK1YcA2DtVR"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = CuDNNGRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wpnPUiV48cca"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "attention_layer = BahdanauAttention(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "mUrLM-bRf57y",
    "outputId": "be1cffe3-6e49-4d71-8e86-5124a30af71a"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7dYaBu78pfU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class Decoder(tf.keras.Model):\\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\\n    super(Decoder, self).__init__()\\n    self.batch_sz = batch_sz\\n    self.dec_units = dec_units\\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\\n    self.gru = tf.keras.layers.GRU(self.dec_units,\\n                                   return_sequences=True,\\n                                   return_state=True,\\n                                   recurrent_initializer='glorot_uniform')\\n    self.fc = tf.keras.layers.Dense(vocab_size)\\n\\n    # used for attention\\n    self.attention = BahdanauAttention(self.dec_units)\\n\\n  def call(self, x, hidden, enc_output):\\n    # enc_output shape == (batch_size, max_length, hidden_size)\\n    context_vector, attention_weights = self.attention(hidden, enc_output)\\n\\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\\n    x = self.embedding(x)\\n\\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\\n\\n    # passing the concatenated vector to the GRU\\n    output, state = self.gru(x)\\n\\n    # output shape == (batch_size * 1, hidden_size)\\n    output = tf.reshape(output, (-1, output.shape[2]))\\n\\n    # output shape == (batch_size, vocab)\\n    x = self.fc(output)\\n\\n    return x, state, attention_weights\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkHzbiweBBKn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\caesa\\AppData\\Local\\Temp\\ipykernel_11864\\2192532245.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "cuDNNGRU is Used\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCv0X9CPBMta"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABtres4SNtUx"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpointss'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XHatiEnCD4bO"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "t84vbnHTExZI",
    "outputId": "8f173d05-b9ca-4e5c-a136-e8ca09f14dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.0528\n",
      "Time taken for 1 batch 17.846041917800903 sec\n",
      "\n",
      "Epoch 1 Batch 100 Loss 2.3312\n",
      "Time taken for 1 batch 30.29406714439392 sec\n",
      "\n",
      "Epoch 1 Batch 200 Loss 1.9118\n",
      "Time taken for 1 batch 49.19191098213196 sec\n",
      "\n",
      "Epoch 1 Batch 300 Loss 1.8919\n",
      "Time taken for 1 batch 76.97880005836487 sec\n",
      "\n",
      "Epoch 1 Batch 400 Loss 2.0025\n",
      "Time taken for 1 batch 108.07027912139893 sec\n",
      "\n",
      "Epoch 1 Batch 500 Loss 1.7567\n",
      "Time taken for 1 batch 125.61813306808472 sec\n",
      "\n",
      "Epoch 1 Batch 600 Loss 1.8734\n",
      "Time taken for 1 batch 137.29990410804749 sec\n",
      "\n",
      "Epoch 1 Batch 700 Loss 1.7788\n",
      "Time taken for 1 batch 149.17096734046936 sec\n",
      "\n",
      "Epoch 1 Batch 800 Loss 1.7665\n",
      "Time taken for 1 batch 160.92014145851135 sec\n",
      "\n",
      "Epoch 1 Batch 900 Loss 1.6349\n",
      "Time taken for 1 batch 172.78545928001404 sec\n",
      "\n",
      "Epoch 1 Batch 1000 Loss 1.5743\n",
      "Time taken for 1 batch 184.6241581439972 sec\n",
      "\n",
      "Epoch 1 Batch 1100 Loss 1.4780\n",
      "Time taken for 1 batch 196.49456977844238 sec\n",
      "\n",
      "Epoch 1 Batch 1200 Loss 1.4017\n",
      "Time taken for 1 batch 210.2466766834259 sec\n",
      "\n",
      "Epoch 1 Batch 1300 Loss 1.5083\n",
      "Time taken for 1 batch 223.3231029510498 sec\n",
      "\n",
      "Epoch 1 Batch 1400 Loss 1.3120\n",
      "Time taken for 1 batch 235.9006724357605 sec\n",
      "\n",
      "Epoch 1 Loss 1.8191\n",
      "Time taken for 1 epoch 236.54041004180908 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3469\n",
      "Time taken for 1 batch 0.17287111282348633 sec\n",
      "\n",
      "Epoch 2 Batch 100 Loss 1.2107\n",
      "Time taken for 1 batch 12.667886018753052 sec\n",
      "\n",
      "Epoch 2 Batch 200 Loss 1.1644\n",
      "Time taken for 1 batch 25.07623863220215 sec\n",
      "\n",
      "Epoch 2 Batch 300 Loss 1.2571\n",
      "Time taken for 1 batch 37.77739191055298 sec\n",
      "\n",
      "Epoch 2 Batch 400 Loss 1.1548\n",
      "Time taken for 1 batch 50.229509115219116 sec\n",
      "\n",
      "Epoch 2 Batch 500 Loss 1.1548\n",
      "Time taken for 1 batch 69.6378059387207 sec\n",
      "\n",
      "Epoch 2 Batch 600 Loss 1.1047\n",
      "Time taken for 1 batch 90.11932182312012 sec\n",
      "\n",
      "Epoch 2 Batch 700 Loss 0.9012\n",
      "Time taken for 1 batch 109.92357230186462 sec\n",
      "\n",
      "Epoch 2 Batch 800 Loss 1.1157\n",
      "Time taken for 1 batch 149.5330033302307 sec\n",
      "\n",
      "Epoch 2 Batch 900 Loss 1.1140\n",
      "Time taken for 1 batch 203.97887778282166 sec\n",
      "\n",
      "Epoch 2 Batch 1000 Loss 0.8453\n",
      "Time taken for 1 batch 257.34564304351807 sec\n",
      "\n",
      "Epoch 2 Batch 1100 Loss 0.8885\n",
      "Time taken for 1 batch 312.4654381275177 sec\n",
      "\n",
      "Epoch 2 Batch 1200 Loss 0.9013\n",
      "Time taken for 1 batch 365.5066645145416 sec\n",
      "\n",
      "Epoch 2 Batch 1300 Loss 0.7938\n",
      "Time taken for 1 batch 419.3165075778961 sec\n",
      "\n",
      "Epoch 2 Batch 1400 Loss 0.9920\n",
      "Time taken for 1 batch 468.6881573200226 sec\n",
      "\n",
      "Epoch 2 Loss 1.0633\n",
      "Time taken for 1 epoch 471.5720703601837 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9174\n",
      "Time taken for 1 batch 0.6254606246948242 sec\n",
      "\n",
      "Epoch 3 Batch 100 Loss 0.8377\n",
      "Time taken for 1 batch 48.6964225769043 sec\n",
      "\n",
      "Epoch 3 Batch 200 Loss 0.7754\n",
      "Time taken for 1 batch 95.44961833953857 sec\n",
      "\n",
      "Epoch 3 Batch 300 Loss 0.7264\n",
      "Time taken for 1 batch 136.21655249595642 sec\n",
      "\n",
      "Epoch 3 Batch 400 Loss 0.8126\n",
      "Time taken for 1 batch 163.2772126197815 sec\n",
      "\n",
      "Epoch 3 Batch 500 Loss 0.7363\n",
      "Time taken for 1 batch 189.6792585849762 sec\n",
      "\n",
      "Epoch 3 Batch 600 Loss 0.6993\n",
      "Time taken for 1 batch 215.07018065452576 sec\n",
      "\n",
      "Epoch 3 Batch 700 Loss 0.7343\n",
      "Time taken for 1 batch 238.50079083442688 sec\n",
      "\n",
      "Epoch 3 Batch 800 Loss 0.6489\n",
      "Time taken for 1 batch 260.9652855396271 sec\n",
      "\n",
      "Epoch 3 Batch 900 Loss 0.6498\n",
      "Time taken for 1 batch 284.36735820770264 sec\n",
      "\n",
      "Epoch 3 Batch 1000 Loss 0.6761\n",
      "Time taken for 1 batch 303.8898255825043 sec\n",
      "\n",
      "Epoch 3 Batch 1100 Loss 0.6023\n",
      "Time taken for 1 batch 318.3559331893921 sec\n",
      "\n",
      "Epoch 3 Batch 1200 Loss 0.8131\n",
      "Time taken for 1 batch 335.8301291465759 sec\n",
      "\n",
      "Epoch 3 Batch 1300 Loss 0.5703\n",
      "Time taken for 1 batch 356.7134232521057 sec\n",
      "\n",
      "Epoch 3 Batch 1400 Loss 0.5149\n",
      "Time taken for 1 batch 375.6869807243347 sec\n",
      "\n",
      "Epoch 3 Loss 0.7018\n",
      "Time taken for 1 epoch 376.3827323913574 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.5141\n",
      "Time taken for 1 batch 0.18831610679626465 sec\n",
      "\n",
      "Epoch 4 Batch 100 Loss 0.3844\n",
      "Time taken for 1 batch 18.59777569770813 sec\n",
      "\n",
      "Epoch 4 Batch 200 Loss 0.5146\n",
      "Time taken for 1 batch 34.394237995147705 sec\n",
      "\n",
      "Epoch 4 Batch 300 Loss 0.5471\n",
      "Time taken for 1 batch 50.144665479660034 sec\n",
      "\n",
      "Epoch 4 Batch 400 Loss 0.5399\n",
      "Time taken for 1 batch 65.94725942611694 sec\n",
      "\n",
      "Epoch 4 Batch 500 Loss 0.5682\n",
      "Time taken for 1 batch 80.38978624343872 sec\n",
      "\n",
      "Epoch 4 Batch 600 Loss 0.4785\n",
      "Time taken for 1 batch 94.74890613555908 sec\n",
      "\n",
      "Epoch 4 Batch 700 Loss 0.5594\n",
      "Time taken for 1 batch 109.14629006385803 sec\n",
      "\n",
      "Epoch 4 Batch 800 Loss 0.4925\n",
      "Time taken for 1 batch 123.55166482925415 sec\n",
      "\n",
      "Epoch 4 Batch 900 Loss 0.5487\n",
      "Time taken for 1 batch 137.9836347103119 sec\n",
      "\n",
      "Epoch 4 Batch 1000 Loss 0.5542\n",
      "Time taken for 1 batch 152.41865253448486 sec\n",
      "\n",
      "Epoch 4 Batch 1100 Loss 0.5719\n",
      "Time taken for 1 batch 167.07242941856384 sec\n",
      "\n",
      "Epoch 4 Batch 1200 Loss 0.5072\n",
      "Time taken for 1 batch 182.28816628456116 sec\n",
      "\n",
      "Epoch 4 Batch 1300 Loss 0.4717\n",
      "Time taken for 1 batch 196.835373878479 sec\n",
      "\n",
      "Epoch 4 Batch 1400 Loss 0.4288\n",
      "Time taken for 1 batch 211.23159790039062 sec\n",
      "\n",
      "Epoch 4 Loss 0.5270\n",
      "Time taken for 1 epoch 211.96465516090393 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.4543\n",
      "Time taken for 1 batch 0.2083747386932373 sec\n",
      "\n",
      "Epoch 5 Batch 100 Loss 0.4033\n",
      "Time taken for 1 batch 15.617421388626099 sec\n",
      "\n",
      "Epoch 5 Batch 200 Loss 0.3654\n",
      "Time taken for 1 batch 29.724348068237305 sec\n",
      "\n",
      "Epoch 5 Batch 300 Loss 0.3987\n",
      "Time taken for 1 batch 43.8583037853241 sec\n",
      "\n",
      "Epoch 5 Batch 400 Loss 0.4717\n",
      "Time taken for 1 batch 57.92120146751404 sec\n",
      "\n",
      "Epoch 5 Batch 500 Loss 0.4728\n",
      "Time taken for 1 batch 71.96823501586914 sec\n",
      "\n",
      "Epoch 5 Batch 600 Loss 0.4997\n",
      "Time taken for 1 batch 86.4302339553833 sec\n",
      "\n",
      "Epoch 5 Batch 700 Loss 0.4330\n",
      "Time taken for 1 batch 101.75380349159241 sec\n",
      "\n",
      "Epoch 5 Batch 800 Loss 0.3967\n",
      "Time taken for 1 batch 115.36718130111694 sec\n",
      "\n",
      "Epoch 5 Batch 900 Loss 0.4013\n",
      "Time taken for 1 batch 128.99861550331116 sec\n",
      "\n",
      "Epoch 5 Batch 1000 Loss 0.4731\n",
      "Time taken for 1 batch 142.52520084381104 sec\n",
      "\n",
      "Epoch 5 Batch 1100 Loss 0.4500\n",
      "Time taken for 1 batch 156.06512236595154 sec\n",
      "\n",
      "Epoch 5 Batch 1200 Loss 0.3628\n",
      "Time taken for 1 batch 169.65068411827087 sec\n",
      "\n",
      "Epoch 5 Batch 1300 Loss 0.4728\n",
      "Time taken for 1 batch 183.22127318382263 sec\n",
      "\n",
      "Epoch 5 Batch 1400 Loss 0.4639\n",
      "Time taken for 1 batch 196.87581133842468 sec\n",
      "\n",
      "Epoch 5 Loss 0.4248\n",
      "Time taken for 1 epoch 197.56956100463867 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.3602\n",
      "Time taken for 1 batch 0.2128298282623291 sec\n",
      "\n",
      "Epoch 6 Batch 100 Loss 0.3740\n",
      "Time taken for 1 batch 15.014897108078003 sec\n",
      "\n",
      "Epoch 6 Batch 200 Loss 0.2969\n",
      "Time taken for 1 batch 28.8282310962677 sec\n",
      "\n",
      "Epoch 6 Batch 300 Loss 0.3056\n",
      "Time taken for 1 batch 42.62220740318298 sec\n",
      "\n",
      "Epoch 6 Batch 400 Loss 0.2915\n",
      "Time taken for 1 batch 56.76443672180176 sec\n",
      "\n",
      "Epoch 6 Batch 500 Loss 0.3420\n",
      "Time taken for 1 batch 71.05589771270752 sec\n",
      "\n",
      "Epoch 6 Batch 600 Loss 0.2520\n",
      "Time taken for 1 batch 84.38695335388184 sec\n",
      "\n",
      "Epoch 6 Batch 700 Loss 0.4196\n",
      "Time taken for 1 batch 97.83174800872803 sec\n",
      "\n",
      "Epoch 6 Batch 800 Loss 0.3473\n",
      "Time taken for 1 batch 111.24356865882874 sec\n",
      "\n",
      "Epoch 6 Batch 900 Loss 0.3641\n",
      "Time taken for 1 batch 124.64464974403381 sec\n",
      "\n",
      "Epoch 6 Batch 1000 Loss 0.3593\n",
      "Time taken for 1 batch 138.08179664611816 sec\n",
      "\n",
      "Epoch 6 Batch 1100 Loss 0.3130\n",
      "Time taken for 1 batch 151.4996325969696 sec\n",
      "\n",
      "Epoch 6 Batch 1200 Loss 0.3139\n",
      "Time taken for 1 batch 164.87724113464355 sec\n",
      "\n",
      "Epoch 6 Batch 1300 Loss 0.3302\n",
      "Time taken for 1 batch 178.26264643669128 sec\n",
      "\n",
      "Epoch 6 Batch 1400 Loss 0.3983\n",
      "Time taken for 1 batch 191.59516596794128 sec\n",
      "\n",
      "Epoch 6 Loss 0.3573\n",
      "Time taken for 1 epoch 192.27169108390808 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.3122\n",
      "Time taken for 1 batch 0.21397829055786133 sec\n",
      "\n",
      "Epoch 7 Batch 100 Loss 0.3319\n",
      "Time taken for 1 batch 13.664546966552734 sec\n",
      "\n",
      "Epoch 7 Batch 200 Loss 0.2808\n",
      "Time taken for 1 batch 27.345860958099365 sec\n",
      "\n",
      "Epoch 7 Batch 300 Loss 0.2793\n",
      "Time taken for 1 batch 41.12699580192566 sec\n",
      "\n",
      "Epoch 7 Batch 400 Loss 0.3041\n",
      "Time taken for 1 batch 54.391493797302246 sec\n",
      "\n",
      "Epoch 7 Batch 500 Loss 0.2750\n",
      "Time taken for 1 batch 67.57937622070312 sec\n",
      "\n",
      "Epoch 7 Batch 600 Loss 0.2630\n",
      "Time taken for 1 batch 80.87540626525879 sec\n",
      "\n",
      "Epoch 7 Batch 700 Loss 0.3508\n",
      "Time taken for 1 batch 94.04002022743225 sec\n",
      "\n",
      "Epoch 7 Batch 800 Loss 0.3786\n",
      "Time taken for 1 batch 107.16454005241394 sec\n",
      "\n",
      "Epoch 7 Batch 900 Loss 0.3229\n",
      "Time taken for 1 batch 120.25638556480408 sec\n",
      "\n",
      "Epoch 7 Batch 1000 Loss 0.2909\n",
      "Time taken for 1 batch 133.33991289138794 sec\n",
      "\n",
      "Epoch 7 Batch 1100 Loss 0.2924\n",
      "Time taken for 1 batch 146.53234577178955 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1200 Loss 0.3113\n",
      "Time taken for 1 batch 159.67333889007568 sec\n",
      "\n",
      "Epoch 7 Batch 1300 Loss 0.2853\n",
      "Time taken for 1 batch 172.82199382781982 sec\n",
      "\n",
      "Epoch 7 Batch 1400 Loss 0.2593\n",
      "Time taken for 1 batch 186.02133989334106 sec\n",
      "\n",
      "Epoch 7 Loss 0.3090\n",
      "Time taken for 1 epoch 186.65465474128723 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.3113\n",
      "Time taken for 1 batch 0.2082068920135498 sec\n",
      "\n",
      "Epoch 8 Batch 100 Loss 0.2740\n",
      "Time taken for 1 batch 13.253719329833984 sec\n",
      "\n",
      "Epoch 8 Batch 200 Loss 0.2587\n",
      "Time taken for 1 batch 26.27681040763855 sec\n",
      "\n",
      "Epoch 8 Batch 300 Loss 0.2991\n",
      "Time taken for 1 batch 39.34469699859619 sec\n",
      "\n",
      "Epoch 8 Batch 400 Loss 0.2792\n",
      "Time taken for 1 batch 52.33340930938721 sec\n",
      "\n",
      "Epoch 8 Batch 500 Loss 0.2748\n",
      "Time taken for 1 batch 65.31745338439941 sec\n",
      "\n",
      "Epoch 8 Batch 600 Loss 0.2427\n",
      "Time taken for 1 batch 78.31164693832397 sec\n",
      "\n",
      "Epoch 8 Batch 700 Loss 0.2936\n",
      "Time taken for 1 batch 91.34318900108337 sec\n",
      "\n",
      "Epoch 8 Batch 800 Loss 0.3951\n",
      "Time taken for 1 batch 104.4387719631195 sec\n",
      "\n",
      "Epoch 8 Batch 900 Loss 0.2575\n",
      "Time taken for 1 batch 117.49726176261902 sec\n",
      "\n",
      "Epoch 8 Batch 1000 Loss 0.3173\n",
      "Time taken for 1 batch 130.47871041297913 sec\n",
      "\n",
      "Epoch 8 Batch 1100 Loss 0.3248\n",
      "Time taken for 1 batch 143.52675557136536 sec\n",
      "\n",
      "Epoch 8 Batch 1200 Loss 0.2834\n",
      "Time taken for 1 batch 156.49468445777893 sec\n",
      "\n",
      "Epoch 8 Batch 1300 Loss 0.2609\n",
      "Time taken for 1 batch 169.66918516159058 sec\n",
      "\n",
      "Epoch 8 Batch 1400 Loss 0.2367\n",
      "Time taken for 1 batch 182.60794734954834 sec\n",
      "\n",
      "Epoch 8 Loss 0.2706\n",
      "Time taken for 1 epoch 183.2649028301239 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2485\n",
      "Time taken for 1 batch 0.22591519355773926 sec\n",
      "\n",
      "Epoch 9 Batch 100 Loss 0.2530\n",
      "Time taken for 1 batch 13.455256700515747 sec\n",
      "\n",
      "Epoch 9 Batch 200 Loss 0.2522\n",
      "Time taken for 1 batch 26.43387484550476 sec\n",
      "\n",
      "Epoch 9 Batch 300 Loss 0.2796\n",
      "Time taken for 1 batch 39.45125651359558 sec\n",
      "\n",
      "Epoch 9 Batch 400 Loss 0.1894\n",
      "Time taken for 1 batch 52.46163892745972 sec\n",
      "\n",
      "Epoch 9 Batch 500 Loss 0.2465\n",
      "Time taken for 1 batch 65.45773673057556 sec\n",
      "\n",
      "Epoch 9 Batch 600 Loss 0.2520\n",
      "Time taken for 1 batch 78.43428468704224 sec\n",
      "\n",
      "Epoch 9 Batch 700 Loss 0.2785\n",
      "Time taken for 1 batch 91.47791004180908 sec\n",
      "\n",
      "Epoch 9 Batch 800 Loss 0.2446\n",
      "Time taken for 1 batch 104.64383673667908 sec\n",
      "\n",
      "Epoch 9 Batch 900 Loss 0.2619\n",
      "Time taken for 1 batch 118.17588567733765 sec\n",
      "\n",
      "Epoch 9 Batch 1000 Loss 0.2442\n",
      "Time taken for 1 batch 131.20912861824036 sec\n",
      "\n",
      "Epoch 9 Batch 1100 Loss 0.2858\n",
      "Time taken for 1 batch 144.20838809013367 sec\n",
      "\n",
      "Epoch 9 Batch 1200 Loss 0.2172\n",
      "Time taken for 1 batch 157.1576864719391 sec\n",
      "\n",
      "Epoch 9 Batch 1300 Loss 0.2447\n",
      "Time taken for 1 batch 170.13544416427612 sec\n",
      "\n",
      "Epoch 9 Batch 1400 Loss 0.2660\n",
      "Time taken for 1 batch 183.22384190559387 sec\n",
      "\n",
      "Epoch 9 Loss 0.2411\n",
      "Time taken for 1 epoch 183.8769874572754 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2354\n",
      "Time taken for 1 batch 0.18508505821228027 sec\n",
      "\n",
      "Epoch 10 Batch 100 Loss 0.2557\n",
      "Time taken for 1 batch 13.15730595588684 sec\n",
      "\n",
      "Epoch 10 Batch 200 Loss 0.2164\n",
      "Time taken for 1 batch 26.19418740272522 sec\n",
      "\n",
      "Epoch 10 Batch 300 Loss 0.1997\n",
      "Time taken for 1 batch 39.133991956710815 sec\n",
      "\n",
      "Epoch 10 Batch 400 Loss 0.1994\n",
      "Time taken for 1 batch 52.14877772331238 sec\n",
      "\n",
      "Epoch 10 Batch 500 Loss 0.2520\n",
      "Time taken for 1 batch 65.10802245140076 sec\n",
      "\n",
      "Epoch 10 Batch 600 Loss 0.1651\n",
      "Time taken for 1 batch 78.25756359100342 sec\n",
      "\n",
      "Epoch 10 Batch 700 Loss 0.2792\n",
      "Time taken for 1 batch 91.21789264678955 sec\n",
      "\n",
      "Epoch 10 Batch 800 Loss 0.2325\n",
      "Time taken for 1 batch 104.18868637084961 sec\n",
      "\n",
      "Epoch 10 Batch 900 Loss 0.2209\n",
      "Time taken for 1 batch 117.24630308151245 sec\n",
      "\n",
      "Epoch 10 Batch 1000 Loss 0.2457\n",
      "Time taken for 1 batch 130.18281292915344 sec\n",
      "\n",
      "Epoch 10 Batch 1100 Loss 0.1954\n",
      "Time taken for 1 batch 143.06157112121582 sec\n",
      "\n",
      "Epoch 10 Batch 1200 Loss 0.2512\n",
      "Time taken for 1 batch 156.03405666351318 sec\n",
      "\n",
      "Epoch 10 Batch 1300 Loss 0.2219\n",
      "Time taken for 1 batch 168.90822625160217 sec\n",
      "\n",
      "Epoch 10 Batch 1400 Loss 0.1627\n",
      "Time taken for 1 batch 181.8723180294037 sec\n",
      "\n",
      "Epoch 10 Loss 0.2173\n",
      "Time taken for 1 epoch 182.51427674293518 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss = total_loss + batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "      print('Time taken for 1 batch {} sec\\n'.format(time.time() - start))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    " # if (epoch + 1) % 2 == 0:\n",
    " #   checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIbkNJWl750m"
   },
   "outputs": [],
   "source": [
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "#checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuFVut942Ko8"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpointss\\\\ckpt-1'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxNPS1aTNI-r"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "import matplotlib.ticker as ticker\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-w3W6IfWxYD7"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "x00g29wuxicN",
    "outputId": "da066001-9d4e-4d45-f092-70f72311c3be",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#translate(u'narai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "xKZXcyVMxvTA",
    "outputId": "40f2926a-0580-46eb-b79e-6bf6eae8a52d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#translate(u'umai lagih belajar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> sapa sing mati <end>\n",
      "Predicted translation: siapa yang mati ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caesa\\AppData\\Local\\Temp\\ipykernel_11864\\2979653732.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "C:\\Users\\caesa\\AppData\\Local\\Temp\\ipykernel_11864\\2979653732.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'sapa sing mati ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt (5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
